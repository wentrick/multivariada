---
title: "Lista Entrega 5"
author: "Davi Wentrick Feijó - 200016806"
date: "2023-06-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(psych,readxl,data.table,Matrix,knitr)
```


### Questao 1 (9.1)

A questao nos dá a matriz de covariancia $\rho$ e a matriz de erros $\Psi$


```{r echo=FALSE}
p = matrix(c(1.0,0.63,0.45,
             0.63,1.0,0.35,
             0.45,0.35,1.0),nrow = 3)

psi = matrix(c(0.19,0,0,0,0.51,0,0,0,0.75),ncol = 3)
```
A matriz $\rho$
```{r echo=FALSE}
kable(p) 
```

A matriz $\Psi$

```{r echo=FALSE}
kable(psi) 
```

Sabemos que na analise fatorial temos a seguinte relacao:

$$
\Sigma = LL^T + \Psi
$$
$$
LL^T = \Sigma - \Psi
$$
Calculando $LL^T$

```{r echo=FALSE}
LLT = p - psi
```

```{r echo=FALSE}
kable(LLT) 
```

Podemos encontrar a comunalidade na diagonal da matriz $LL^T$ já que subtraimos o $\Psi$

```{r echo=FALSE}
comu = diag(LLT)
```

```{r echo=FALSE}
kable(comu)
```

Com essas informacoes podemos escrever nossa matriz $\Sigma$ como:

$$
\Sigma = LL^T + \Psi
$$
```{r}
p_construido = LLT + psi
```

```{r echo=FALSE}
kable(p_construido)
```


### Questao 2 (9.2)

#### A)

As comunalidades sao:

```{r}
comu
```
Podemos perceber que F1 detem a maior comunalidade logo é o fator que mais explica a variancia dos dados

#### B)
Sabemos que:
$$
Cor(X,Y) = \frac{Cov(X,Y)}{S_xS_y}
$$
$$
Cov(X,F) = L
$$
Logo

$$
Cor(X_i,F_i) = \frac{Cov(X_i,F_i)}{S_iS_f} = \frac{L_i}{S_xS_f}
$$
```{r echo=FALSE}
eigen_p = eigen(p)


autoval <- eigen_p$values



autovet <- eigen_p$vectors


D <- matrix(0, nrow = 3, ncol = 3)
diag(D) <- sqrt(autoval)

Lestimado <- (autovet%*%D)[,1]
LTestimado <- t(Lestimado)

LLT <- Lestimado%*%LTestimado

comu <- Lestimado^2

psiestimado <- diag(p-LLT)

ajuste <- p - (LLT+psi)

var_explained = autoval/sum(autoval)
```



```{r}
cor_xf = Lestimado[1]/(1*comu[1])
```

```{r echo=FALSE}
cor_xf
```


\newpage
### Questao 3 (9.3)

#### A)
Para realizar por meio de componentes principais primeiro precisamos encontrar os autovalores e autovetores da matriz de correlacao aplicando a decompisicao espectral em $\rho$ dada na questao 9.1

$$
\rho = CDC^T
$$

```{r}
eigen_p = eigen(p)

autoval <- eigen_p$values

autovet <- eigen_p$vectors

D <- matrix(0, nrow = 3, ncol = 3)
diag(D) <- sqrt(autoval)
```


```{r}
autoval
```

```{r}
autovet
```


Em seguida podemos encontrar nossa matriz L

$$
L = CD^{1/2}
$$

```{r echo=FALSE}
Lestimado <- (autovet%*%D)[,1]

LTestimado <- t(Lestimado)

LLT <- Lestimado%*%LTestimado
```

Aqui temos nossa matriz dos loadings
```{r}
Lestimado
```

Para calcular a matriz $\Psi$ temos que seguir a equacao:

$$
 \Psi = \Sigma - LL^T
$$
Na diagonal obteremos nosso $\Psi$

```{r}
psiestimado <- diag(p-LLT)
```


```{r}
psiestimado
```

Para comparar com os resultados anteriores podemos aproximar a matrix $\Sigma$ de correlacoes por meio da formula:

$$
\Sigma = LL^T + \Psi
$$

```{r echo=FALSE}
sigma_aprox <- LLT+psi
```

```{r echo=FALSE}
sigma_aprox
```

#### B)

A variancia explicada é:


```{r echo=FALSE}
var_explained = autoval/sum(autoval)
```

```{r echo=FALSE}
var_explained
```

Podemos notar que a primeira componente exxplica 65% da variancia dos dados

\newpage
### Questao 4 (9.19)

Para essa seção estaremos trabalhando com a seguinte matriz de correlação:
```{r include=FALSE}
names = c("x1","x2","x3","x4","x5","x6","x7")
data <- read_excel("table9_12-SalespeopleData.xlsx", 
                                        col_names = names, col_types = c("numeric", 
                                                                         "numeric", "numeric", "numeric", 
                                                                         "numeric", "numeric", "numeric"))
cor_data = cor(as.matrix(data))
```

```{r echo=FALSE}
kable(cor_data)
```



#### A)

Vamos usar a função principa() para obter a analise fatorial com m=2 e m=3
```{r echo=FALSE}
AF2 <- principal(cor_data, nfactors = 2, rotate = 'none',
                   covar = F,n.obs = 50)
AF3 <- principal(cor_data, nfactors = 3, rotate = 'none',
                 covar = F,n.obs = 50)
```

m=2
```{r}
AF2
```
Vale notar que a primeira componente explica 72% enquanto que a segunda 13%. Isso indica que adicionar uma 3 variavel nao deve adicionar muita informação

\newpage
m=3
```{r}
AF3
```

Como haviamos discutido antes, acabou que a 3 componente adicionou 7%

\newpage
#### B)
```{r echo=FALSE}
#b)
AF2_rotated <- principal(cor_data, nfactors = 2, rotate = 'varimax',
                 covar = F,n.obs = 50)
AF3_rotated <- principal(cor_data, nfactors = 3, rotate = 'varimax',
                 covar = F,n.obs = 50)
```

Aqui temos as mesmas analises porem rotacionadas com o metodos "varimax"
```{r}
AF2_rotated
```

\newpage
```{r}
AF3_rotated
```
Apos aplicar a rotação podemos perceber que a informação fica mais bem distribuida entre as componentes. O objetivo é simplificar a analise.

\newpage
#### C)
```{r echo=FALSE}
#c)

AF2_comu = AF2$loadings^2
AF2_psi = diag(AF2$uniquenesses)
AF2_LLT = AF2$loadings %*% t(AF2$loadings)
resAF2 <- round(cor(data) - AF2_LLT - AF2_psi,3)


AF3_comu = AF3$loadings^3
AF3_psi = diag(AF3$uniquenesses)
AF3_LLT = AF3$loadings %*% t(AF3$loadings)
resAF3 <- round(cor(data) - AF3_LLT - AF3_psi,3)
```

Vamos obter as comunalidades, a variancia especifica e a matriz L sem a rotação varimax

```{r echo=FALSE}
cat("Resultados para m=2")
cat("A comunalidade é:\n",round(AF2_comu,2))
cat("A diagonal da matriz psi é:\n",round(AF2$uniquenesses,2))
cat("A matriz LLT:")
round(AF2_LLT,3)
cat("Matriz de correlação aproximada:")
resAF2
```
```{r echo=FALSE}
cat("Resultados para m=3")
cat("A comunalidade é:\n",round(AF3_comu,2))
cat("A diagonal da matriz psi é:\n",round(AF3$uniquenesses,2))
cat("A matriz LLT:")
round(AF3_LLT,3)
cat("Matriz de correlação aproximada:")
resAF2
```





#### D)

Vamos testar se o numero de fatores é suficiente para representar o banco

- Hipótese nula ($H_0$): $\Sigma = LL^T + \Psi$
- Hipótese alternativa ($H_1$): $\Sigma \neq LL^T + \Psi$

Podemos usar a propria função principal para fazer o calculo do teste, mas para isso temos que adicionar o numero de observaçoes.
```{r echo=FALSE}
#d)

#m=2

n = dim(data)[1]
p = dim(AF2$loadings)[1]
m = dim(AF2$loadings)[2]

AF2_teste_stat = AF2$chi
AF2_pvalue = AF2$PVAL
```

```{r echo=FALSE}
cat("Teste para m=2")
cat('A estatica qui-quadrado obtida foi:',AF2_teste_stat)
cat('O p-valor obtido foi:',AF2_pvalue)
```


```{r echo=FALSE}
#m=3

n = dim(data)[1]
p = dim(AF3$loadings)[1]
m = dim(AF3$loadings)[2]

AF3_teste_stat = AF3$chi
AF3_pvalue = AF3$PVAL
```

```{r echo=FALSE}
cat("Teste para m=3")
cat('A estatica qui-quadrado obtida foi:',AF3_teste_stat)
cat('O p-valor obtido foi:',AF3_pvalue)
```


A partir do p-valor temos evidencias para rejeitar H0 indicando que 2 e 3 fatores nao sao suficiente para estimar a matriz $\Sigma$ logo teriamos que usar mais fatores para representar melhor.


