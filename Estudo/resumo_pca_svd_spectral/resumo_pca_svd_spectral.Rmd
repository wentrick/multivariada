---
title: "Decomposição Espectral, SVD e PCA"
author: "Davi Wentrick Feijó - 200016806"
date: "2023-05-23"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse,knitr,kableExtra)
set.seed(420)
```

### Decomposição Espectral

A decomposição espectral consiste em decompor uma matriz A no produto de 3 matrizes da seguinte forma:

$$
A_{n \times n} = P_{n \times n} D_{n \times n} P_{n \times n}^{T}
$$


Onde P é uma matriz onde as cada coluna é composta por um autovetor de A e D e uma matriz diagonal com os autovalores correspondentes a cada autovetor de A.

Podemos realizar isso dentro do R por meio da função eigen(). Vamos utilizar a seguinte matriz e usaremos a matriz de covariancia dela (com isso garantimos que ela é positiva definida)

```{r echo=FALSE}
dados <- matrix(round(runif(16,min = 0,max = 30), 0),nrow = 4)
A = cov(dados)
A
```

Aqui vamos fazer nossa decomposicao, o resultado saira como um objeto onde \$vectors é nosso P e \$values é a matriz D (temos que usar a função diag() para que tenhamos a matriz diagona pois o resultado sai em um vetor)
```{r}
spectral_decomposition = eigen(A)
```

```{r}
P = spectral_decomposition$vectors
```

```{r echo=FALSE}
P
```

```{r}
D = diag(spectral_decomposition$values)
```

```{r echo=FALSE}
D
```

Podemos confimar a decomposição tentando obter a matriz A novamente

```{r}
A_recuperado = P %*% D %*% solve(P)
```

```{r echo=FALSE}
A_recuperado
```

Vale notar que quando A é simetrica $P^{-1} = P^{T}$

```{r}
P_transposto = t(P)
```

```{r echo=FALSE}
P_transposto
```

```{r}
P_inversa = solve(P)
```

```{r echo=FALSE}
P_inversa
```

\newpage

### Single Value Decomposition (SVD)

O SVD assim como a decomposição espectral busca representar uma matriz A pelo produto de 3 matrizes da seguinte forma:

$$
A_{m \times n} = U_{m \times n} \Sigma_{n \times n} V^{T}_{n \times n}
$$

Onde U e V sao matrizes ortogonais e $\Sigma$ (ou D ou S) é uma matriz diagonal.O cálculo da SVD consiste em encontrar os autovalores e autovetores de $AA^T$ e $A^TA$. Os autovetores de $A^TA$ compõem as colunas de V, os autovetores de $AA^T$ compõem as colunas de U. Além disso, os valores singulares em S (Matriz de valores singulares) são raízes quadradas dos autovalores de $AA^T$ ou $A^TA$. Os valores singulares são as entradas diagonais da matriz S e estão dispostos em ordem decrescente. Os valores singulares são sempre números reais. Se a matriz A for uma matriz real, então U e V também são reais. Resumindo:

$$
U = AA^T
$$

$$
V = A^TA
$$
$$
S = \text{Raiz quadrada dos autovalores de U ou V} 
$$
Vale notar que ao aplicar essas transformações nós estamos rotacionando os dados (ortonormalidade) com U, escalando os vetores com S e rotacionando novamente (voltando ao estado inicial) com $V^T$.

```{r}
sv_decomp = svd(A)
```


```{r echo=FALSE}
sv_decomp
```

Podemos verificar o SVD tentando recuparar a matriz A
```{r}
sv_decomp$u %*% diag(sv_decomp$d) %*% t(sv_decomp$v)
```


### Diferenças entre a Decomposição espectral e o SVD

Considere a decomposição em autovalores $A=PDP^{-1}$ e a SVD $A=U \Sigma V^{-1}$. Algumas diferenças-chave são as seguintes:

* Os vetores na matriz de decomposição em autovalores P não são necessariamente ortogonais, então a mudança de base não é uma simples rotação. Por outro lado, os vetores nas matrizes U e V na SVD são ortonormais, então eles representam rotações (e possivelmente reflexões). 

* Na SVD, as matrizes não diagonais U e V não são necessariamente inversas uma da outra. Geralmente, elas não têm relação entre si. Na decomposição em autovalores, as matrizes não diagonais P e $P^{-1}$ são inversas uma da outra.

* Na SVD, as entradas na matriz diagonal $\Sigma$ são todas números reais e não negativos. Na decomposição em autovalores, as entradas de D podem ser qualquer número complexo - negativo, positivo, imaginário, qualquer coisa.

* A SVD sempre existe para qualquer tipo de matriz retangular ou quadrada, enquanto a decomposição em autovalores só existe para matrizes quadradas e, mesmo entre as matrizes quadradas, às vezes ela não existe.


### Analise de componentens principais (PCA)

Aplicar o SVD em uma matriz de Covariancia ou Correlação podemos retirar informacoes importantes sobre um conjunto de dados.

Mas antes vamos entender o que ocorre com os dados ao obter essas matrizes.

* A matriz de covariancia ela mantem a escala dos dados e não é centralizada.

* A matriz de correlaçao ela normaliza os dados (divide pela variancia) ou seja ela controla a escala das variaveis deixando todas "iguais" 

* A centralização dos dados pode ser feita, que nada mais é que subtrair a media de cada coluna, centralizando os dados em torno de 0 é recomendado ser feita.

A PCA pode ser feita tanto por Decomposição espectral ou SVD. A diferenca é que o SVD funciona em matrizes nao quadradas como vimos antes. Podemos comparar os resultados obtidos manualmente com as funcoes ja implementadas.

Podemos mostrar que a matriz de covariancia A centralizada é:

$$
A_{n \times n} = \frac{X_{n \times m}  X_{n \times m}^{T}}{n-1}
$$
Podemos aplicar a decomposicao espectral em X e susbtituir na formula
$$
X_{m \times n} = U_{m \times n} \Sigma_{n \times n} V^{T}_{n \times n}
$$
$$
A_{n \times n} = \frac{V \Sigma U^{T} U \Sigma V^{T}}{n-1}
$$
$$
A_{n \times n} = V\frac{\Sigma^2 }{n-1}V^{T}
$$
Com isso sabemos a relacao entre os valores singulares (diagonal do SVD) e os autovalores (Decomposicao espectral)


$$
\lambda = \frac{\Sigma^2 }{n-1}
$$


```{r}
A = scale(A,center = T)
n = dim(A)[1]
# Executando a PCA com prcomp()
resultado_prcomp <- prcomp(A)

# Executando a decomposição SVD com svd()
resultado_svd <- svd(A)

# Executando a decomposição espectral com eigen()
resultado_eigen <- eigen(A)

# Obtendo os componentes principais e autovalores usando prcomp()
autovetores_prcomp <- resultado_prcomp$rotation
autovalores_prcomp <- resultado_prcomp$sdev^2
componentes_principais_prcomp = resultado_prcomp$x

# Obtendo os componentes principais e autovalores usando a decomposição SVD manual
autovetores_svd <- resultado_svd$v
autovalores_svd <- (resultado_svd$d^2/(n-1))
componentes_principais_svd = A %*% resultado_svd$v

# Obtendo os componentes principais e autovalores usando a decomposição SVD manual
autovetores_eigen <- resultado_eigen$vectors
autovalores_eigen <- resultado_eigen$values
componentes_principais_eigen = A %*% resultado_eigen$vectors
```

Vamos comparar os 3 metodos observando os resultados obtidos

Podemos perceber que uma vez que trabalhamos com os dados centralizados todos os metodos obtem os mesmos autovetores (rotacao)
```{r echo=FALSE}

print("Autovetores (Rotacao):")
cat("Prcomp:")
print(autovetores_prcomp)
print("SVD Manual:")
print(autovetores_svd)
print("Eigen:")
print(autovetores_svd)
```

Observando os autovalores (nosso escalar que indica a maior variancia) pode ser obtido. Vale notar que o prcomp nos dá o desvio padrao (entao elevamos ao quadrado para obter a variancia), o svd temos a diagonal com os valores singulares, para obter os autovalores é necessario elevar ao quadrado e dividir por N ou n-1 no caso de amostra. 
```{r echo=FALSE}
# Comparando os autovalores
print("Autovalores:")
cat("Prcomp:",round(autovalores_prcomp,3))
cat("SVD Manual:",round(autovalores_svd,3))
cat("Eigen:",round(autovalores_eigen,3))
```

As componentes principais nada mais é do que a projecao da matriz A de covariancia no plano de maior variancia para isso temos que realizar a seguinte conta:

$$
PC_k = A \times P_k
$$
Onde A é sua matriz original (no nosso caso é a matriz de covariancias A) e P é a matriz de autovetores (rotacao) obtida. Ao relizar essa multiplicacao teremos as $k$ componentes principais


```{r echo=FALSE}
# Comparando os componentes principais
print("Componentes Principais (Rotacao):")
cat("Prcomp:")
print(componentes_principais_prcomp)
print("SVD Manual:")
print(componentes_principais_svd)
print("Eigen:")
print(componentes_principais_svd)
```










